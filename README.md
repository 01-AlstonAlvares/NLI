# ğŸ§  Sentence-BERT (SBERT) for Natural Language Inference

A from-scratch **BERT model** fine-tuned using a **Siamese Sentence-BERT architecture** for **Natural Language Inference (NLI)** with real-time web prediction.

---

## âœ¨ Project Overview

This repository presents a full pipeline implementation of:

* ğŸ”§ Building BERT from scratch
* ğŸ” Converting it into a Siamese SBERT architecture
* ğŸ“Š Training on large-scale datasets using streaming
* ğŸŒ Deploying a Flask web app for live inference

The system predicts the semantic relationship between two sentences:

| Relationship      | Meaning                                   |
| ----------------- | ----------------------------------------- |
| **Entailment**    | Hypothesis logically follows from premise |
| **Neutral**       | No clear relationship                     |
| **Contradiction** | Hypothesis contradicts premise            |

---

## ğŸ—ï¸ Architecture Highlights

### ğŸ”¹ BERT from Scratch

Implemented a full multi-layered Transformer Encoder including:

* Multi-Head Self Attention
* Scaled Dot-Product Attention
* Positional Encoding
* Feed-Forward Networks
* Layer Normalization & Residual Connections

### ğŸ”¹ Siamese SBERT Structure

Two identical BERT encoders share weights:

```
Premise â”€â”€â–º BERT â”€â”€â–º Sentence Embedding (u)
Hypothesis â”€â–º BERT â”€â–º Sentence Embedding (v)
                      â”‚
                      â–¼
                Classification Layer
```

This enables efficient sentence similarity and NLI prediction.

---

## ğŸ¯ Training Objective

Model is fine-tuned using a **Softmax classification loss** to predict:

```
Entailment | Neutral | Contradiction
```

---

## ğŸ“š Dataset Credits & Sources

To maintain computational feasibility, **100,000 samples** from each dataset were used.

| Dataset                                       | Purpose                  | Source                                                                                 |
| --------------------------------------------- | ------------------------ | -------------------------------------------------------------------------------------- |
| ğŸ“– Wikipedia (20220301.en)                    | BERT Pre-training subset | [https://huggingface.co/datasets/wikipedia](https://huggingface.co/datasets/wikipedia) |
| ğŸ§ª SNLI (Stanford Natural Language Inference) | Siamese Fine-tuning      | [https://nlp.stanford.edu/projects/snli/](https://nlp.stanford.edu/projects/snli/)     |

### âš¡ Streaming Data Processing

Wikipedia dataset (>20GB) is processed using **HuggingFace Streaming Mode**, allowing on-the-fly loading without large disk usage.

---

## ğŸ“Š Performance Report

Evaluation on **1,000 SNLI validation samples** after **5 training epochs**:

| Class            | Precision | Recall | F1-Score | Support |
| ---------------- | --------- | ------ | -------- | ------- |
| Entailment       | 0.76      | 0.76   | 0.76     | 331     |
| Neutral          | 0.66      | 0.70   | 0.68     | 333     |
| Contradiction    | 0.75      | 0.71   | 0.73     | 336     |
| **Accuracy**     |           |        | **0.72** | 1000    |
| **Macro Avg**    | 0.73      | 0.72   | 0.72     | 1000    |
| **Weighted Avg** | 0.73      | 0.72   | 0.72     | 1000    |

ğŸ† **Overall Accuracy: 72%**

---

## ğŸŒ Web Application

A simple **Flask interface** allows real-time NLI predictions.

### ğŸ’¡ Features

* Enter Premise and Hypothesis
* Get instant classification result
* Lightweight & local deployment

---

## âš™ï¸ How to Run Locally

### 1ï¸âƒ£ Install Dependencies

```
pip install torch transformers datasets flask scikit-learn tqdm
```

### 2ï¸âƒ£ Launch Application

```
python main.py
```

### 3ï¸âƒ£ Open Web Interface

Visit ğŸ‘‰ [http://127.0.0.1:5000](http://127.0.0.1:5000)

---

## ğŸš€ Key Achievements

âœ”ï¸ Implemented BERT without prebuilt libraries
âœ”ï¸ Built SBERT Siamese architecture
âœ”ï¸ Used streaming for large-scale data handling
âœ”ï¸ Achieved 72% accuracy on SNLI validation
âœ”ï¸ Deployed an interactive inference web app

---

## âš ï¸ Problems / Limitations Faced

The most significant challenge was managing the **computational and storage overhead** required for a deep BERT architecture.

* Full Wikipedia + SNLI datasets exceed **20GB**, requiring a shift to **streaming mode**.
* Scaling to deeper layers increased **training time and VRAM usage**.
* Batch sizes had to be tuned carefully to avoid **GPU Out-of-Memory errors**.
* Dataset noise and conflicting labels in validation caused errors during performance reporting and required cleaning.

---

## ğŸ–¥ï¸ Web App Screenshots

![Web App Screenshot](https://github.com/user-attachments/assets/a96b1510-424d-403c-bb85-a42d09402b02)

![Web App Screenshot](https://github.com/user-attachments/assets/0931d4b0-7745-46ea-ad1f-5c0e2bda7ff8)

---

## ğŸ‘¨â€ğŸ’»

**Alston Alvares (st126488)**
